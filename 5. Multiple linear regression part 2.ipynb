{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter estimation for linear regression with multiple variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have observations of butyrate production (denoted as 'y') that we hypothesize to depend linearly on the abundances of *multiple* species, where we have $m$ different species to consider. We can write this dependence as, \n",
    "\n",
    "$$\n",
    "y = w_1 x_1 + ... + w_m x_m = \\sum_{j = 1}^m w_j  x_j\n",
    "$$\n",
    "where $w_1, ..., w_m$ is now a vector of unknown parameters that we have to find. \n",
    "\n",
    "If we have $n$ observations of both the species abundance and the resulting butyrate production, then we change the notation to show the prediction of observation $i$ where $i = 1, ..., n$. \n",
    "\n",
    "$$\n",
    "y_i = \\sum_{j = 1}^m w_j x_{ij} \n",
    "$$\n",
    "\n",
    "We now need to find the optimal values for all of the parameters, $w_1, ..., w_m$. The optimal values can still be found using the same sum of squares objective function:\n",
    "\n",
    "$$\n",
    "(w_1, ..., w_m)^* = \\underset{w_1, ..., w_m}{\\text{argmin}} \\sum_{i = 1}^n \\left( y_i - \\sum_{j = 1}^m w_j x_{ij} \\right)^2 \n",
    "$$\n",
    "\n",
    "This problem can be solved analytically, in the same way as in the single variable case, though it's very challenging to tackle without using a more convenient notation.  \n",
    "\n",
    "The above equations can be written in a more compact form using matrix-vector notation. Vectors such as the vector of unknown parameters are written using bold script, e.g. $\\mathbf{w}^T = [w_1, ..., w_m]$. Similarly, the vector of species abundances can be written as $\\mathbf{x}^T = [x_1, ..., x_m]$. The convention is that all vectors are column vectors and the transpose operation turns the vector into a row vector. \n",
    "\n",
    "$$\n",
    "\\mathbf{x} = \n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\ \\vdots \\\\ x_m\n",
    "\\end{bmatrix} \n",
    "\\qquad \n",
    "\\mathbf{x}^T = \n",
    "\\begin{bmatrix}\n",
    "x_1, \\ldots, x_m\n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "The summation of parameters and species abundances is the same as the *dot product*:\n",
    "$$\n",
    "\\mathbf{w}^T \\cdot \\mathbf{x} = \\mathbf{x}^T \\cdot \\mathbf{w} = w_1 x_1 + ... + w_m  x_m = \\sum_{j = 1}^m w_j x_j\n",
    "$$\n",
    "\n",
    "Now the prediction of $y$ can be written as a dot product\n",
    "\n",
    "$$\n",
    "y = \\mathbf{x}^T \\cdot \\mathbf{w}\n",
    "$$\n",
    "\n",
    "Since we have $n$ observations of $y$, we can write them as an $n$ dimensional vector, $y_1, ..., y_n = \\mathbf{y}$. Now we can write a single equation that includes all samples and all parameters \n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \n",
    "\\begin{bmatrix}\n",
    "y_1 \\\\ \\vdots \\\\ y_n \n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{x}_1^T \\cdot \\mathbf{w} \\\\ \\vdots \\\\ \\mathbf{x}_n^T \\cdot \\mathbf{w}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{x}_1^T \\\\ \\vdots \\\\ \\mathbf{x}_n^T \n",
    "\\end{bmatrix}\n",
    "\\cdot \\mathbf{w}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "x_{11} \\ldots x_{1 m} \\\\ \\vdots \\ddots \\vdots  \\\\ x_{n1} \\ldots x_{nm}\n",
    "\\end{bmatrix}\n",
    "\\cdot \\mathbf{w}\n",
    "=\n",
    "\\mathbf{X} \\cdot \\mathbf{w}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{X} \\cdot \\mathbf{w}\n",
    "$$\n",
    "\n",
    "The above expression introduces the matrix, $\\mathbf{X}$, where an element of the matrix, $x_{ij}$, is the abundance of species $j$ in sample $i$. Matrices are conventionally noted as boldface capitalized variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For practice, try writing this sum of squares objective function using matrix-vector notation\n",
    "\n",
    "$$\n",
    "g(\\mathbf{w}) = \\sum_{i = 1}^n \\left( y_i - \\sum_{j = 1}^m w_j x_{ij} \\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result:\n",
    "\n",
    "$$\n",
    "g(\\mathbf{w}) = (\\mathbf{y} - \\mathbf{X} \\cdot \\mathbf{w})^T \\cdot (\\mathbf{y} - \\mathbf{X} \\cdot \\mathbf{w})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve for $\\mathbf{w}$ that minimizes the objective function\n",
    "\n",
    "Similar to the single variable case, we want to find \n",
    "\n",
    "$$\n",
    "\\mathbf{w}^* = \\underset{\\mathbf{w}}{\\text{argmin}}  \\; g(\\mathbf{w})\n",
    "$$\n",
    "\n",
    "We can still approach this problem by taking the derivative of $g(\\mathbf{w})$ with respect to $\\mathbf{w}$, setting the result equal to zero, and solving for $\\mathbf{w}^*$, however the math is more difficult because $\\mathbf{w}^*$ is an $m$ dimensional vector. \n",
    "\n",
    "The vector version of the derivative is called a *gradient*, which is denoted using the $\\nabla$ symbol. This symbol is used as a shorthand for the derivative of $g(\\mathbf{w})$ with respect to each element of $\\mathbf{w}$, \n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{w}} g (\\mathbf{w}) = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial}{\\partial w_1} g(\\mathbf{w}) \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial w_m} g(\\mathbf{w})\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To approach taking the gradient of $g(\\mathbf{w})$, it will be helpful to start by expanding the polynomial, which requires some careful matrix algebra. \n",
    "\n",
    "$$\n",
    "g(\\mathbf{w}) = (\\mathbf{y} - \\mathbf{X} \\cdot \\mathbf{w})^T \\cdot (\\mathbf{y} - \\mathbf{X} \\cdot \\mathbf{w})\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \n",
    "\\mathbf{y}^T \\cdot \\mathbf{y} - 2 \\mathbf{w}^T \\cdot \\mathbf{X}^T \\cdot \\mathbf{y} + \\mathbf{w}^T \\cdot \\mathbf{X}^T \\cdot \\mathbf{X} \\cdot \\mathbf{w}\n",
    "$$\n",
    "\n",
    "To do the above matrix algebra, we had to use the following identity to take the transpose of the product of two matrices: $(\\mathbf{A} \\cdot \\mathbf{B})^T = \\mathbf{B}^T \\cdot \\mathbf{A}^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same way that the derivative of a sum is the same as the sum of derivatives, the gradient of a sum is also the sum of gradients. This means that the gradient of the above expression is the sum of the gradient of its parts: \n",
    "\n",
    "$$ \\nabla_{\\mathbf{w}} g (\\mathbf{w}) = \n",
    "\\nabla_{\\mathbf{w}} ( \\mathbf{y}^T \\cdot \\mathbf{y} - 2 \\mathbf{w}^T \\cdot \\mathbf{X}^T \\cdot \\mathbf{y} + \\mathbf{w}^T \\cdot \\mathbf{X}^T \\cdot \\mathbf{X} \\cdot \\mathbf{w} ) \n",
    "$$\n",
    "\n",
    "$$ \\nabla_{\\mathbf{w}} g (\\mathbf{w}) = \n",
    "\\nabla_{\\mathbf{w}} ( \\mathbf{y}^T \\cdot \\mathbf{y}) - \\nabla_{\\mathbf{w}} (2 \\mathbf{w}^T \\cdot \\mathbf{X}^T \\cdot \\mathbf{y}) + \\nabla_{\\mathbf{w}} (\\mathbf{w}^T \\cdot \\mathbf{X}^T \\cdot \\mathbf{X} \\cdot \\mathbf{w} )\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating each gradient separately "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the first term, $\\mathbf{y}^T \\cdot \\mathbf{y}$, does not depend on $\\mathbf{w}$, the gradient is zero. \n",
    "\n",
    "To approach taking the gradient of an expression like $2 \\mathbf{w}^T \\cdot \\mathbf{X}^T \\cdot \\mathbf{y}$, it's helpful to first think about the dimensions of each of the terms so that we can determine the dimension of the gradient. \n",
    "\n",
    "For example, the dimension of $\\mathbf{w}^T$ is $1 \\times m$, the dimension of $\\mathbf{X}^T$ is $m \\times n$, and the dimension of $\\mathbf{y}$ is $n \\times 1$. This means that the matrix product has dimension $1 \\times 1$, so it's a just a single scalar value. \n",
    "\n",
    "Taking the gradient of scalar valued function with respect to $\\mathbf{w}$ will result in a vector, since the components of the gradient are the derivatives of the scalar valued function with respect to each element of $\\mathbf{w}$.  \n",
    "\n",
    "Written as an equation this looks like, \n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{w}} (2 \\mathbf{w}^T \\cdot \\mathbf{X}^T \\cdot \\mathbf{y}) \n",
    "= \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial}{\\partial w_1} (2 \\mathbf{w}^T \\cdot \\mathbf{X}^T \\cdot \\mathbf{y}) \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial}{\\partial w_m} (2 \\mathbf{w}^T \\cdot \\mathbf{X}^T \\cdot \\mathbf{y})\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the gradient vector involves first determining a general expression for an element of the gradient, \n",
    "\n",
    "We can write element $k$  of the vector $\\nabla_{\\mathbf{w}} (2 \\mathbf{w}^T \\cdot \\mathbf{X}^T \\cdot \\mathbf{y})$ as\n",
    "\n",
    "$$\n",
    "[\\nabla_{\\mathbf{w}} (2 \\mathbf{w}^T \\cdot \\mathbf{X}^T \\cdot \\mathbf{y})]_k = \n",
    "\\frac{\\partial}{\\partial w_k} (2 \\mathbf{w}^T \\cdot \\mathbf{X}^T \\cdot \\mathbf{y})\n",
    "$$\n",
    "\n",
    "It's helpful now to convert the above equation back to index notation, \n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w_k} \\left( 2 \\sum_{i=1}^m w_i \\sum_{j=1}^n [\\mathbf{X}^T]_{ij} y_j \\right)\n",
    "$$\n",
    "\n",
    "Using $[\\mathbf{X}^T]_{ij} = x_{ji}$ and pulling the derivative inside of the summation gives, \n",
    "\n",
    "$$\n",
    "= 2 \\sum_{i=1}^m \\frac{\\partial w_i}{\\partial w_k} \\sum_{j=1}^n y_j x_{ji}\n",
    "$$\n",
    "\n",
    "The term $\\frac{\\partial w_i}{\\partial w_k}$ is the derivative of regression coefficient $i$ with respect to regression coefficient $k$. When $i$ is not equal to $k$, the derivative is zero, and when $i$ does equal $k$, the derivative is one since it's the derivative of the coefficient with respect to itself. This is so common an occurence in vector math that there's a special variable called the Kronecker delta to handle this scenario,  \n",
    "\n",
    "$$\n",
    "\\delta_{ik} = \n",
    "\\begin{cases}\n",
    "      1, & \\text{if}\\ i=k \\\\\n",
    "      0, & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "2 \\sum_{i=1}^m \\frac{\\partial w_i}{\\partial w_k} \\sum_{j=1}^n y_j x_{ji} \n",
    "$$\n",
    "\n",
    "$$\n",
    "= 2 \\sum_{i=1}^m \\delta_{ik} \\sum_{j=1}^n y_j x_{ji} \n",
    "$$\n",
    "\n",
    "Because the Kronecker delta is zero unless $i = k$, the only elements of $x_{ji}$ left after performing the first summation from $i = 1, ..., m$ is $x_{jk}$,\n",
    "\n",
    "$$\n",
    "= 2 \\sum_{j=1}^n y_j x_{jk}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 2 \\sum_{j=1}^n [\\mathbf{X}]^T_{kj} y_j \n",
    "$$\n",
    "\n",
    "Converting back to vector notation, \n",
    "\n",
    "$$\n",
    "[\\nabla_{\\mathbf{w}} \\left( 2 \\mathbf{w}^T \\cdot \\mathbf{X}^T \\cdot \\mathbf{y} \\right) ]_k\n",
    "= 2 [\\mathbf{X}^T \\mathbf{y}]_k\n",
    "$$\n",
    "\n",
    "Since the above equation is true for any index $k$, we can remove the index. \n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{w}} \\left( 2 \\mathbf{w}^T \\cdot \\mathbf{X}^T \\cdot \\mathbf{y} \\right)\n",
    "= 2 \\mathbf{X}^T \\mathbf{y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we need to take the gradient of the last term \n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{w}} (\\mathbf{w}^T \\cdot \\mathbf{X}^T \\cdot \\mathbf{X} \\cdot \\mathbf{w} )\n",
    "$$\n",
    "\n",
    "Try this on your own as an exercise. As a hint, remember to use the product rule when taking the derivative of a product. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In index notation (dropping the summation symbols is called \"Einstein notation\"), \n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w_i} w_j X_{jk}^T X_{kl} w_l\n",
    "=\n",
    "X_{jk}^T X_{kl} \\frac{\\partial}{\\partial w_i} w_j w_l\n",
    "$$\n",
    "\n",
    "Using the product rule, \n",
    "\n",
    "$$\n",
    "=\n",
    "X_{jk}^T X_{kl} \\left( \\frac{\\partial w_j}{\\partial w_i} w_l + w_j \\frac{\\partial w_l}{\\partial w_i} \\right)\n",
    "$$\n",
    "\n",
    "Using the same logic from before to replace the derivatives with Kronecker deltas, \n",
    "\n",
    "$$\n",
    "=\n",
    "X_{jk}^T X_{kl} \\left( \\delta_{ji} w_l + w_j \\delta_{li} \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\n",
    "X_{kj} X_{kl} \\left( \\delta_{ji} w_l + w_j \\delta_{li} \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\n",
    "X_{kj} \\delta_{ji} X_{kl} w_l + X_{kj} X_{kl} \\delta_{li} w_j \n",
    "$$\n",
    "\n",
    "$$\n",
    "=\n",
    "X_{ki} X_{kl} w_l + X_{ki} X_{kj} w_j \n",
    "$$\n",
    "\n",
    "$$\n",
    "=\n",
    "X_{ik}^T X_{kl} w_l + X_{ik}^T X_{kj} w_j \n",
    "$$\n",
    "\n",
    "$$\n",
    "=\n",
    "2 X_{ik}^T X_{kl} w_l \n",
    "$$\n",
    "\n",
    "Converting back to vector notation, \n",
    "\n",
    "$$\n",
    "[\\nabla_{\\mathbf{w}} (\\mathbf{w}^T \\cdot \\mathbf{X}^T \\cdot \\mathbf{X} \\cdot \\mathbf{w} )]_i\n",
    "= \n",
    "2 X_{ik}^T X_{kl} w_l\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{w}} (\\mathbf{w}^T \\cdot \\mathbf{X}^T \\cdot \\mathbf{X} \\cdot \\mathbf{w} )\n",
    "= \n",
    "2 \\mathbf{X}^T \\mathbf{X} \\mathbf{w}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the derived expressions for the gradients, \n",
    "\n",
    "$$ \\nabla_{\\mathbf{w}} g (\\mathbf{w}) = \n",
    "\\nabla_{\\mathbf{w}} ( \\mathbf{y}^T \\cdot \\mathbf{y}) - \\nabla_{\\mathbf{w}} (2 \\mathbf{w}^T \\cdot \\mathbf{X}^T \\cdot \\mathbf{y}) + \\nabla_{\\mathbf{w}} (\\mathbf{w}^T \\cdot \\mathbf{X}^T \\cdot \\mathbf{X} \\cdot \\mathbf{w} )\n",
    "$$\n",
    "\n",
    "$$ \\nabla_{\\mathbf{w}} g (\\mathbf{w}) = \n",
    "-2 \\mathbf{X}^T \\mathbf{y}\n",
    "+\n",
    "2 \\mathbf{X}^T \\mathbf{X} \\mathbf{w}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to find the $\\mathbf{w}^*$ that minimizes the objective function, so we can set the above expression for the gradient of $g(\\mathbf{w})$ to zero and solve for $\\mathbf{w}^*$, \n",
    "\n",
    "$$\n",
    "-2 \\mathbf{X}^T \\mathbf{y}\n",
    "+\n",
    "2 \\mathbf{X}^T \\mathbf{X} \\mathbf{w}^* = 0 \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{X}^T \\mathbf{y} = \n",
    "\\mathbf{X}^T \\mathbf{X} \\mathbf{w}^*\n",
    "$$\n",
    "\n",
    "In order to solve for $\\mathbf{w}^*$, we need to invert the matrix $\\mathbf{X}^T \\mathbf{X}$. On a technical aside, matrices are not always invertible. In order for a matrix to be invertible, it must be a square, *full rank* matrix. A matrix is full rank if all of its rows and columns are linearly independent. For now we'll assume that $\\mathbf{X}^T \\mathbf{X}$ is invertible. \n",
    "\n",
    "If we multiply the left and right hand sides of the equation by $[\\mathbf{X}^T \\mathbf{X} ]^{-1}$, we get \n",
    "\n",
    "$$\n",
    "[\\mathbf{X}^T \\mathbf{X} ]^{-1} \\mathbf{X}^T \\mathbf{y} = \n",
    "[\\mathbf{X}^T \\mathbf{X} ]^{-1} \\mathbf{X}^T \\mathbf{X} \\mathbf{w}^*\n",
    "$$\n",
    "\n",
    "$$\n",
    "[\\mathbf{X}^T \\mathbf{X} ]^{-1} \\mathbf{X}^T \\mathbf{y} = \\mathbf{w}^*\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative approach "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{y} = \\mathbf{X} \\cdot \\mathbf{w} \n",
    "$$\n",
    "\n",
    "Left multiply both sides of the equation by $\\mathbf{X}^T$, \n",
    "\n",
    "$$\n",
    "\\mathbf{X}^T \\mathbf{y} = \\mathbf{X}^T \\mathbf{X} \\cdot \\mathbf{w} \n",
    "$$\n",
    "\n",
    "Left multiply both sides of the equation by $[\\mathbf{X}^T \\mathbf{X}]^{-1}$, \n",
    "\n",
    "$$\n",
    "[\\mathbf{X}^T \\mathbf{X}]^{-1} \\cdot \\mathbf{X}^T \\mathbf{y} = \\mathbf{w}^*\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
